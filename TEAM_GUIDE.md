
# ðŸ§¬ Longevity Biomarker Tracking SystemÂ â€” Team Guide (v0.4)

Welcome to the repo!Â This page is the **sourceâ€‘ofâ€‘truth for who owns what, what lives where, and the 5â€‘minute routine to spin everything up locally.**

---

## 1Â Â·Â Team roles & areas of ownership

| Â RoleÂ                                     | Â Main goalsÂ                                                  | Â Owns these pathsÂ                                     |
| ----------------------------------------- | ------------------------------------------------------------ | ----------------------------------------------------- |
| **Database ArchitectÂ (@dbâ€‘architect)**    | Â â€¢ keep the schema inÂ 3Â NF Â â€¢ publish the ER diagram         | Â `/sql/`,Â `/docs/er_diagram.*`                        |
| **Data EngineerÂ (@dataâ€‘engineer)**        | Â â€¢ pull NHANES XPTs Â â€¢ clean â†’ CSVs Â â€¢ bulkâ€‘load into MySQL  | Â `/etl/`,Â `/data/raw/`,Â `/data/clean/`,Â `/notebooks/` |
| **Backend LeadÂ (@backendâ€‘lead)**          | Â â€¢ convert FastAPI stubs to real endpoints Â â€¢ add unit tests | Â `/src/api/`,Â `/tests/`                               |
| **AnalyticsÂ /Â UI LeadÂ (@analyticsâ€‘lead)** | Â â€¢ build Streamlit dashboard Â â€¢ charts & tables              | Â `/src/ui/`,Â `/src/analytics/`                        |

> **Branch etiquette**Â Â â†’Â *One featureÂ = one branch.* Open a PR, request review from anyone whose code you touch. **CI must be green** before merge.

---

## 2Â Â·Â Repository tour (birdsâ€‘eye view)

```
â”œâ”€â”€ sql/                 â† DDL (schema + seed data)
â”œâ”€â”€ etl/                 â† downloadÂ â†’Â transformÂ â†’Â load pipeline
â”‚Â Â  â”œâ”€ download_nhanes.py
â”‚Â Â  â”œâ”€ transform.ipynb   â† âš ï¸ placeholder
â”‚Â Â  â””â”€ load.sh
â”œâ”€â”€ data/
â”‚Â Â  â”œâ”€ raw/    â† large XPTs (gitâ€‘ignored)
â”‚Â Â  â””â”€ clean/  â† CSVs ready for LOADÂ INFILE (gitâ€‘ignored)
â”œâ”€â”€ src/
â”‚Â Â  â”œâ”€ api/          â† FastAPI app
â”‚Â Â  â”œâ”€ analytics/    â† helper modules for stats / plots
â”‚Â Â  â””â”€ ui/           â† Streamlit dashboard
â”œâ”€â”€ tests/           â† pytest suite
â”œâ”€â”€ docker-compose.yml  â† MySQLÂ 8 + Adminer
â”œâ”€â”€ Makefile          â† oneâ€‘word workflows (`make db`, â€¦)
â”œâ”€â”€ .env.example      â† copyÂ â†’Â .env ; local config lives here
â”œâ”€â”€ .github/workflows/ci.yml â† GitHubÂ Actions (DB + API + tests)
â””â”€â”€ docs/             â† diagrams & additional docs
```

### 2bÂ Â·Â Fileâ€‘byâ€‘file cheatâ€‘sheet

| Path                       | What it is / Why it matters                                                                                                             |
| -------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| `sql/schema.sql`           | Single sourceâ€‘ofâ€‘truth DDL + seed inserts. All migrations are manual edits to this file.                                                |
| `etl/download_nhanes.py`   | Pulls raw NHANES XPT files for the 2017â€‘2018 cycle to `data/raw/`.                                                                      |
| `etl/transform.ipynb`      | **TODO:** Data Engineer cleans & joins XPTs â†’ four CSVs in `data/clean/`. Notebook executes headless via `nbconvert` during `make etl`. |
| `etl/load.sh`              | Uses `LOAD DATA LOCAL INFILE` to bulkâ€‘insert the clean CSVs. Also writes a tiny relational sample dump for CI.                          |
| `docker-compose.yml`       | Spins up MySQL 8 (portÂ 3307) + Adminer 4 (portÂ 8080). DB initialises from `sql/` at first boot.                                         |
| `Makefile`                 | Quick commands: `make db`, `make etl`, `make run`, `make ui`, `make test`,Â etc. The `run` target now defaults to `127.0.0.1:8000`.      |
| `src/api/main.py`          | FastAPI entryâ€‘point (currently returns stub JSON). Hotâ€‘reload via `make run`.                                                           |
| `src/ui/app.py`            | Streamlit dashboard scaffolding; checks API health on the Home page.                                                                    |
| `tests/`                   | `conftest.py` sets up DB + API fixtures. `test_api.py` hits stub endpoints to ensure the stack is alive.                                |
| `.github/workflows/ci.yml` | GitHub Action: spins up MySQL service â†’ loads schema & sample data â†’ runs tests â†’ schema diff check to catch unâ€‘committed DDL edits.    |
| `.pre-commit-config.yaml`  | Black, flake8, trailing whitespace, YAML linting. Autoâ€‘installed by `make install`.                                                     |
| `codebase_snapshot.sh`     | Utility to create a gistâ€‘friendly snapshot excluding big binariesâ€”no need to run during normal dev.                                     |

---

## 3Â Â·Â Getting started locallyÂ (â‰ˆÂ 5Â minutes)

> **Prereqs:** DockerÂ Desktop, PythonÂ 3.11+, `pip install pre-commit`.

```bash
#Â 1Â Â·Â Clone
git clone https://github.com/<org>/longevity-biomarker-tracker
cd longevity-biomarker-tracker

#Â 2Â Â·Â Personal env vars
cp .env.example .env   # edit only if you need custom ports

#Â 3Â Â·Â Install deps & git hooks
make install           # pip + preâ€‘commit

#Â 4Â Â·Â Launch database (MySQL :3307) + Adminer (:8080)
make db

#Â 5Â Â·Â (â€¯DataÂ Engineerâ€¯) download â†’ transform â†’ load sample data
make etl               # safe to run; skips if CSVs missing

#Â 6Â Â·Â Fire up services
make run               # FastAPI hotâ€‘reload on http://127.0.0.1:8000
make ui                # Streamlit on http://127.0.0.1:8501

#Â 7Â Â·Â Sanity check tests & style
pytest -q              # all dots
pre-commit run --all-files
```

---

## 4Â Â·Â Daily workflow cheatsheet

| Action                         | Command                          |
| ------------------------------ | -------------------------------- |
| Start DB + API + UI            | `make db && make run && make ui` |
| Reset schema after SQL edits   | `make db-reset`                  |
| Download / refresh NHANES      | `make etl`                       |
| Run all tests                  | `make test`                      |
| Lint & format                  | `pre-commit run --all-files`     |
| Stop everything & wipe volumes | `make clean`                     |

---

## 5Â Â·Â Adminer login (GUI DB client)

* **URL:** [http://localhost:8080](http://localhost:8080)
* **System:** MySQL
* **Server:** `db` (inside docker)Â orÂ `127.0.0.1:3307`
* **User:** `biomarker_user`Â Â Â **PW:**Â `biomarker_pass`
* **Database:**Â `longevity`

---

## 6Â Â·Â FAQ (extended)

<details><summary><strong>Why do I need a PR just to push a notebook?</strong></summary>
Because CI runs on every PR. That guarantees the DB schema, tests, and code style stay in sync. A quick notebook tweak can still break the build if it changes repoâ€‘wide importsâ€”better to catch it before merge.
</details>

<details><summary><strong>Can I use React or Shiny instead of Streamlit?</strong></summary>
Sure!  The UI lead should feel free to use their preferred tools.  Just mention on Slack and please build in the following minimal
code to show that it connects to the rest of the repo.
1. Adds the minimal scaffold for the new tool.
2. Passes preâ€‘commit + pytest + CI.
3. Includes a README note on how to run it.
</details>

<details><summary><strong>Help! Another service is already using portÂ 3307 / 8000 / 8501.</strong></summary>
Edit `.env`, change `MYSQL_PORT`, `APP_API_PORT`, or Streamlitâ€™s port in `Makefile` (`make ui` target). Then restart `make db` / `make run`.
</details>

<details><summary><strong>Preâ€‘commit reformats my fileâ€”whatâ€™s the policy?</strong></summary>
Anything autoâ€‘changed by preâ€‘commit (black, flake8Â fixes) should simply be committed. If you disagree with a rule, open a Discussion with an example.
</details>

<details><summary><strong>How do we add a Python dependency?</strong></summary>
1. Add it to `requirements.txt` (pin exact version).
2. `pip install -r requirements.txt` and ensure `pre-commit run --all-files` is still green.
3. Push a PR; CI will build a fresh environment and verify nothing breaks.
</details>

<details><summary><strong>Tests need real data but the CSVs are big. Whatâ€™s the strategy?</strong></summary>
`load.sh` creates a tiny relational dump (`tests/sample_dump.sql`) with <Â 1Â MB of data selected from the latest 10 users. CI restores that to keep test runs fast.
</details>

---

Letâ€™s build something greatâ€”happy coding! ðŸŽ‰
