{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e22da06",
   "metadata": {},
   "source": [
    "## XPT TO CSV Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d122e",
   "metadata": {},
   "source": [
    "### read XPTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0775d37e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:28:21.862035Z",
     "iopub.status.busy": "2025-05-28T01:28:21.861605Z",
     "iopub.status.idle": "2025-05-28T01:28:22.512090Z",
     "shell.execute_reply": "2025-05-28T01:28:22.506413Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "raw_data_dir = data_dir / \"raw\"\n",
    "clean_data_dir = data_dir / \"clean\"\n",
    "\n",
    "clean_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def read_xpt(file_name, limit_cols=None):\n",
    "    xpt_file_path = raw_data_dir / file_name\n",
    "    df = pd.read_sas(xpt_file_path, format='xport')\n",
    "    if limit_cols:\n",
    "        cols_to_keep = ['SEQN'] + [col for col in limit_cols if col != 'SEQN']\n",
    "        existing_cols_to_keep = [col for col in cols_to_keep if col in df.columns]\n",
    "        missing_cols = set(cols_to_keep) - set(existing_cols_to_keep)\n",
    "        if missing_cols:\n",
    "                print(f\"Warning: For {file_name}, specified columns not found and will be ignored: {missing_cols}\")\n",
    "        return df[existing_cols_to_keep]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e11e91c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:28:22.515441Z",
     "iopub.status.busy": "2025-05-28T01:28:22.515299Z",
     "iopub.status.idle": "2025-05-28T01:28:22.544335Z",
     "shell.execute_reply": "2025-05-28T01:28:22.544120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read           SEQN  SDDSRVYR  RIDSTATR  RIAGENDR  RIDAGEYR  RIDAGEMN  RIDRETH1  \\\n",
      "0      93703.0      10.0       2.0       2.0       2.0       NaN       5.0   \n",
      "1      93704.0      10.0       2.0       1.0       2.0       NaN       3.0   \n",
      "2      93705.0      10.0       2.0       2.0      66.0       NaN       4.0   \n",
      "3      93706.0      10.0       2.0       1.0      18.0       NaN       5.0   \n",
      "4      93707.0      10.0       2.0       1.0      13.0       NaN       5.0   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "9249  102952.0      10.0       2.0       2.0      70.0       NaN       5.0   \n",
      "9250  102953.0      10.0       2.0       1.0      42.0       NaN       1.0   \n",
      "9251  102954.0      10.0       2.0       2.0      41.0       NaN       4.0   \n",
      "9252  102955.0      10.0       2.0       2.0      14.0       NaN       4.0   \n",
      "9253  102956.0      10.0       2.0       1.0      38.0       NaN       3.0   \n",
      "\n",
      "      RIDRETH3  RIDEXMON  RIDEXAGM  ...  DMDHREDZ  DMDHRMAZ  DMDHSEDZ  \\\n",
      "0          6.0       2.0      27.0  ...       3.0       1.0       3.0   \n",
      "1          3.0       1.0      33.0  ...       3.0       1.0       2.0   \n",
      "2          4.0       2.0       NaN  ...       1.0       2.0       NaN   \n",
      "3          6.0       2.0     222.0  ...       3.0       1.0       2.0   \n",
      "4          7.0       2.0     158.0  ...       2.0       1.0       3.0   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "9249       6.0       2.0       NaN  ...       2.0       1.0       1.0   \n",
      "9250       1.0       2.0       NaN  ...       2.0       2.0       NaN   \n",
      "9251       4.0       1.0       NaN  ...       2.0       2.0       NaN   \n",
      "9252       4.0       2.0     175.0  ...       2.0       1.0       2.0   \n",
      "9253       3.0       2.0       NaN  ...       2.0       2.0       NaN   \n",
      "\n",
      "          WTINT2YR      WTMEC2YR  SDMVPSU  SDMVSTRA  INDHHIN2  INDFMIN2  \\\n",
      "0      9246.491865   8539.731348      2.0     145.0      15.0      15.0   \n",
      "1     37338.768343  42566.614750      1.0     143.0      15.0      15.0   \n",
      "2      8614.571172   8338.419786      2.0     145.0       3.0       3.0   \n",
      "3      8548.632619   8723.439814      2.0     134.0       NaN       NaN   \n",
      "4      6769.344567   7064.609730      1.0     138.0      10.0      10.0   \n",
      "...            ...           ...      ...       ...       ...       ...   \n",
      "9249  16896.276203  18338.711104      2.0     138.0       4.0       4.0   \n",
      "9250  61630.380013  63661.951573      2.0     137.0      12.0      12.0   \n",
      "9251  17160.895269  17694.783346      1.0     144.0      10.0      10.0   \n",
      "9252  14238.445922  14871.839636      1.0     136.0       9.0       9.0   \n",
      "9253  38645.740291  39426.299948      1.0     142.0       7.0       7.0   \n",
      "\n",
      "      INDFMPIR  \n",
      "0         5.00  \n",
      "1         5.00  \n",
      "2         0.82  \n",
      "3          NaN  \n",
      "4         1.88  \n",
      "...        ...  \n",
      "9249      0.95  \n",
      "9250       NaN  \n",
      "9251      1.18  \n",
      "9252      2.24  \n",
      "9253      1.56  \n",
      "\n",
      "[9254 rows x 46 columns]\n",
      "First 5 rows of the DEMO_J.XPT data:\n",
      "      SEQN  SDDSRVYR  RIDSTATR  RIAGENDR  RIDAGEYR  RIDAGEMN  RIDRETH1  \\\n",
      "0  93703.0      10.0       2.0       2.0       2.0       NaN       5.0   \n",
      "1  93704.0      10.0       2.0       1.0       2.0       NaN       3.0   \n",
      "2  93705.0      10.0       2.0       2.0      66.0       NaN       4.0   \n",
      "3  93706.0      10.0       2.0       1.0      18.0       NaN       5.0   \n",
      "4  93707.0      10.0       2.0       1.0      13.0       NaN       5.0   \n",
      "\n",
      "   RIDRETH3  RIDEXMON  RIDEXAGM  ...  DMDHREDZ  DMDHRMAZ  DMDHSEDZ  \\\n",
      "0       6.0       2.0      27.0  ...       3.0       1.0       3.0   \n",
      "1       3.0       1.0      33.0  ...       3.0       1.0       2.0   \n",
      "2       4.0       2.0       NaN  ...       1.0       2.0       NaN   \n",
      "3       6.0       2.0     222.0  ...       3.0       1.0       2.0   \n",
      "4       7.0       2.0     158.0  ...       2.0       1.0       3.0   \n",
      "\n",
      "       WTINT2YR      WTMEC2YR  SDMVPSU  SDMVSTRA  INDHHIN2  INDFMIN2  INDFMPIR  \n",
      "0   9246.491865   8539.731348      2.0     145.0      15.0      15.0      5.00  \n",
      "1  37338.768343  42566.614750      1.0     143.0      15.0      15.0      5.00  \n",
      "2   8614.571172   8338.419786      2.0     145.0       3.0       3.0      0.82  \n",
      "3   8548.632619   8723.439814      2.0     134.0       NaN       NaN       NaN  \n",
      "4   6769.344567   7064.609730      1.0     138.0      10.0      10.0      1.88  \n",
      "\n",
      "[5 rows x 46 columns]\n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9254 entries, 0 to 9253\n",
      "Data columns (total 46 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   SEQN      9254 non-null   float64\n",
      " 1   SDDSRVYR  9254 non-null   float64\n",
      " 2   RIDSTATR  9254 non-null   float64\n",
      " 3   RIAGENDR  9254 non-null   float64\n",
      " 4   RIDAGEYR  9254 non-null   float64\n",
      " 5   RIDAGEMN  597 non-null    float64\n",
      " 6   RIDRETH1  9254 non-null   float64\n",
      " 7   RIDRETH3  9254 non-null   float64\n",
      " 8   RIDEXMON  8704 non-null   float64\n",
      " 9   RIDEXAGM  3433 non-null   float64\n",
      " 10  DMQMILIZ  6004 non-null   float64\n",
      " 11  DMQADFC   561 non-null    float64\n",
      " 12  DMDBORN4  9254 non-null   float64\n",
      " 13  DMDCITZN  9251 non-null   float64\n",
      " 14  DMDYRSUS  1948 non-null   float64\n",
      " 15  DMDEDUC3  2306 non-null   float64\n",
      " 16  DMDEDUC2  5569 non-null   float64\n",
      " 17  DMDMARTL  5569 non-null   float64\n",
      " 18  RIDEXPRG  1110 non-null   float64\n",
      " 19  SIALANG   9254 non-null   float64\n",
      " 20  SIAPROXY  9254 non-null   float64\n",
      " 21  SIAINTRP  9254 non-null   float64\n",
      " 22  FIALANG   8780 non-null   float64\n",
      " 23  FIAPROXY  8780 non-null   float64\n",
      " 24  FIAINTRP  8780 non-null   float64\n",
      " 25  MIALANG   6684 non-null   float64\n",
      " 26  MIAPROXY  6684 non-null   float64\n",
      " 27  MIAINTRP  6684 non-null   float64\n",
      " 28  AIALANGA  4977 non-null   float64\n",
      " 29  DMDHHSIZ  9254 non-null   float64\n",
      " 30  DMDFMSIZ  9254 non-null   float64\n",
      " 31  DMDHHSZA  9254 non-null   float64\n",
      " 32  DMDHHSZB  9254 non-null   float64\n",
      " 33  DMDHHSZE  9254 non-null   float64\n",
      " 34  DMDHRGND  9254 non-null   float64\n",
      " 35  DMDHRAGZ  9254 non-null   float64\n",
      " 36  DMDHREDZ  8764 non-null   float64\n",
      " 37  DMDHRMAZ  9063 non-null   float64\n",
      " 38  DMDHSEDZ  4751 non-null   float64\n",
      " 39  WTINT2YR  9254 non-null   float64\n",
      " 40  WTMEC2YR  9254 non-null   float64\n",
      " 41  SDMVPSU   9254 non-null   float64\n",
      " 42  SDMVSTRA  9254 non-null   float64\n",
      " 43  INDHHIN2  8763 non-null   float64\n",
      " 44  INDFMIN2  8780 non-null   float64\n",
      " 45  INDFMPIR  8023 non-null   float64\n",
      "dtypes: float64(46)\n",
      "memory usage: 3.2 MB\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "df_demo_raw = read_xpt(\"DEMO_J.XPT\")\n",
    "print(f\"Successfully read {df_demo_raw}\")\n",
    "print(\"First 5 rows of the DEMO_J.XPT data:\")\n",
    "print(df_demo_raw.head())\n",
    "\n",
    "# Display some information about the DataFrame\n",
    "print(\"\\nDataFrame Info:\")\n",
    "df_demo_raw.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec912ef",
   "metadata": {},
   "source": [
    "### Convert the XPTs into CSV\n",
    "\n",
    "Consult: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/DEMO_J.htm#RIDRETH1\n",
    "\n",
    "For the reference information about the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fa25d4",
   "metadata": {},
   "source": [
    "##### demo xpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e253b57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:28:22.545649Z",
     "iopub.status.busy": "2025-05-28T01:28:22.545531Z",
     "iopub.status.idle": "2025-05-28T01:28:22.576034Z",
     "shell.execute_reply": "2025-05-28T01:28:22.575806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully created and saved users.csv to ../data/clean/users.csv\n",
      "      SEQN   Age Sex                        RaceEthnicity\n",
      "0  93703.0   2.0   F                   Non-Hispanic Asian\n",
      "1  93704.0   2.0   M                   Non-Hispanic White\n",
      "2  93705.0  66.0   F                   Non-Hispanic Black\n",
      "3  93706.0  18.0   M                   Non-Hispanic Asian\n",
      "4  93707.0  13.0   M  Other Race - Including Multi-Racial\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9254 entries, 0 to 9253\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   SEQN           9254 non-null   float64\n",
      " 1   Age            9254 non-null   float64\n",
      " 2   Sex            9254 non-null   object \n",
      " 3   RaceEthnicity  9254 non-null   object \n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 289.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# pre process CSV\n",
    "import numpy as np # For np.nan if needed for missing values\n",
    "\n",
    "df_demo_raw = read_xpt(\"DEMO_J.XPT\")\n",
    "\n",
    "if not df_demo_raw.empty:\n",
    "    # Select and Rename Columns\n",
    "\n",
    "    required_nhanes_cols = {\n",
    "        'SEQN': 'SEQN',\n",
    "        'RIDAGEYR': 'Age',\n",
    "        'RIAGENDR': 'Sex_Code',\n",
    "        'RIDRETH3': 'RaceEthnicity_Code'\n",
    "    }\n",
    "\n",
    "    # Check if all required columns exist\n",
    "    missing_cols = [col for col in required_nhanes_cols.keys() if col not in df_demo_raw.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Error: Missing expected columns in DEMO_J.XPT: {missing_cols}\")\n",
    "        # Handle error appropriately, e.g., skip this transformation\n",
    "    else:\n",
    "        df_users = df_demo_raw[list(required_nhanes_cols.keys())].copy()\n",
    "        df_users.rename(columns=required_nhanes_cols, inplace=True)\n",
    "\n",
    "        # Transform Data\n",
    "\n",
    "        # Recode Sex (RIAGENDR: 1=Male, 2=Female)\n",
    "        sex_map = {1: 'M', 2: 'F'}\n",
    "        df_users['Sex'] = df_users['Sex_Code'].map(sex_map)\n",
    "\n",
    "        # Recode race\n",
    "        race_map = {\n",
    "            1: 'Mexican American',\n",
    "            2: 'Other Hispanic',\n",
    "            3: 'Non-Hispanic White',\n",
    "            4: 'Non-Hispanic Black',\n",
    "            6: 'Non-Hispanic Asian', # skips 5\n",
    "            7: 'Other Race - Including Multi-Racial' # Or simply 'Other'\n",
    "        }\n",
    "        df_users['RaceEthnicity'] = df_users['RaceEthnicity_Code'].map(race_map)\n",
    "\n",
    "        # Select final columns for users.csv and handle missing values\n",
    "        # (UserID is auto-generated by the database)\n",
    "        final_user_columns = ['SEQN', 'Age', 'Sex', 'RaceEthnicity']\n",
    "        df_users_final = df_users[final_user_columns].copy()\n",
    "\n",
    "        # Drop rows if SEQN is missing (it shouldn't be) or if essential fields like Sex are missing after mapping\n",
    "        df_users_final.dropna(subset=['SEQN', 'Sex', 'RaceEthnicity', 'Age'], inplace=True)\n",
    "\n",
    "        # Save to CSV\n",
    "        clean_data_dir = Path(\"../data/clean\")\n",
    "        clean_data_dir.mkdir(parents=True, exist_ok=True) # Ensure the directory exists\n",
    "        users_csv_path = clean_data_dir / \"users.csv\"\n",
    "\n",
    "        df_users_final.to_csv(users_csv_path, index=False)\n",
    "        print(f\"\\nSuccessfully created and saved users.csv to {users_csv_path}\")\n",
    "        print(df_users_final.head())\n",
    "        df_users_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "596742ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:28:22.577087Z",
     "iopub.status.busy": "2025-05-28T01:28:22.577012Z",
     "iopub.status.idle": "2025-05-28T01:28:22.580354Z",
     "shell.execute_reply": "2025-05-28T01:28:22.580128Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set exam date\n",
    "if 'RIDEXMON' in df_demo_raw.columns:\n",
    "    df_exam_month_info = df_demo_raw[['SEQN', 'RIDEXMON']].copy()\n",
    "APPROX_EXAM_DATE = \"2018-01-01\"\n",
    "\n",
    "def assign_exam_date(row):\n",
    "        if pd.isna(row['RIDEXMON']):\n",
    "            return APPROX_EXAM_DATE\n",
    "\n",
    "        year = \"2018\" # Simplified: Defaulting to the latter year of the cycle\n",
    "\n",
    "        if row['RIDEXMON'] == 1.0: # Nov-Apr window\n",
    "            # Let's represent this as Jan of the later year of the cycle.\n",
    "            return f\"2018-01-15\"\n",
    "        elif row['RIDEXMON'] == 2.0: # May-Oct window\n",
    "            # Let's represent this as July of the later year of the cycle.\n",
    "            return f\"2018-07-15\"\n",
    "        return APPROX_EXAM_DATE\n",
    "\n",
    "def populate_date(df, date_name):\n",
    "     if not df_demo_raw.empty and 'RIDEXMON' in df_demo_raw.columns and 'SEQN' in df_demo_raw.columns:\n",
    "        df_exam_month_info = df_demo_raw[['SEQN', 'RIDEXMON']].copy()\n",
    "        # Merge RIDEXMON information into df\n",
    "        df = pd.merge(df, df_exam_month_info, on='SEQN', how='left')\n",
    "        \n",
    "        # Apply the function to create the date_name column\n",
    "        # The `axis=1` tells pandas to apply the function row-wise\n",
    "        df[date_name] = df.apply(assign_exam_date, axis=1)\n",
    "        \n",
    "        # Convert the date_name to the desired string format\n",
    "        df[date_name] = pd.to_datetime(df[date_name]).dt.strftime('%Y-%m-%d')\n",
    "        print(\"Applied assign_exam_date() using RIDEXMON.\")\n",
    "        \n",
    "        # Clean up RIDEXMON if it was merged and no longer needed directly in sessions.csv\n",
    "        if 'RIDEXMON' in df.columns:\n",
    "            df.drop(columns=['RIDEXMON'], inplace=True)\n",
    "        return df[date_name]\n",
    "     else:\n",
    "        # Fallback if RIDEXMON or df_demo_raw is not available\n",
    "        df[date_name] = APPROX_EXAM_DATE\n",
    "        df[date_name] = pd.to_datetime(df[date_name]).dt.strftime('%Y-%m-%d')\n",
    "        print(\"RIDEXMON not available or df_demo_raw missing; using general APPROX_EXAM_DATE for date_name.\")\n",
    "        return df[date_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9687ebc",
   "metadata": {},
   "source": [
    "##### BMX_J\n",
    "\n",
    "Consult: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/BMX_J.htm#RIDRETH1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d8f740",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:28:22.581372Z",
     "iopub.status.busy": "2025-05-28T01:28:22.581296Z",
     "iopub.status.idle": "2025-05-28T01:28:22.633519Z",
     "shell.execute_reply": "2025-05-28T01:28:22.633307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied assign_exam_date() using RIDEXMON.\n",
      "\n",
      "Successfully created and saved anthropometry.csv to ../data/clean/anthropometry.csv\n",
      "Anthropometry.csv has 8704 rows.\n"
     ]
    }
   ],
   "source": [
    "# --- Processing BMX_J.XPT for anthropometry.csv ---\n",
    "# Relevant BMX_J variables: SEQN, BMXWT, BMXHT, BMXBMI\n",
    "df_bmx_raw = read_xpt(\"BMX_J.XPT\", limit_cols=['SEQN', 'BMXWT', 'BMXHT', 'BMXBMI'])\n",
    "\n",
    "if not df_bmx_raw.empty and 'SEQN' in df_bmx_raw.columns:\n",
    "    df_anthropometry = df_bmx_raw.copy()\n",
    "    \n",
    "    # Rename columns to match schema\n",
    "    rename_map_anthro = {\n",
    "        'BMXWT': 'WeightKG',\n",
    "        'BMXHT': 'HeightCM',\n",
    "        'BMXBMI': 'BMI'\n",
    "    }\n",
    "    # Only rename columns that exist\n",
    "    existing_rename_cols = {k: v for k, v in rename_map_anthro.items() if k in df_anthropometry.columns}\n",
    "    df_anthropometry.rename(columns=existing_rename_cols, inplace=True)\n",
    "    \n",
    "    df_anthropometry['ExamDate'] = populate_date(df_anthropometry, 'ExamDate')\n",
    "\n",
    "    # Select and order final columns\n",
    "    # UserID in the database will be based on SEQN. We load SEQN here.\n",
    "    final_anthro_columns = ['SEQN', 'ExamDate', 'HeightCM', 'WeightKG', 'BMI']\n",
    "    \n",
    "    # Filter for columns that actually exist in the DataFrame after renaming and adding\n",
    "    existing_final_anthro_cols = [col for col in final_anthro_columns if col in df_anthropometry.columns]\n",
    "    df_anthropometry_final = df_anthropometry[existing_final_anthro_cols]\n",
    "    \n",
    "    # Drop rows where SEQN is missing, or all measurement values are missing\n",
    "    df_anthropometry_final.dropna(subset=['SEQN'], inplace=True)\n",
    "\n",
    "    if not df_anthropometry_final.empty:\n",
    "        anthropometry_csv_path = clean_data_dir / \"anthropometry.csv\"\n",
    "        df_anthropometry_final.to_csv(anthropometry_csv_path, index=False)\n",
    "        print(f\"\\nSuccessfully created and saved anthropometry.csv to {anthropometry_csv_path}\")\n",
    "        print(f\"Anthropometry.csv has {len(df_anthropometry_final)} rows.\")\n",
    "    else:\n",
    "        print(\"Could not generate anthropometry.csv due to missing columns or data.\")\n",
    "else:\n",
    "    print(\"Skipping anthropometry.csv creation as BMX_J.XPT could not be read or SEQN is missing.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951b9c8f",
   "metadata": {},
   "source": [
    "#### Sessions\n",
    "\n",
    "Consult: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/FASTQX_J.htm#SDDSRVYR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63d3103c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:28:22.634709Z",
     "iopub.status.busy": "2025-05-28T01:28:22.634612Z",
     "iopub.status.idle": "2025-05-28T01:28:22.683656Z",
     "shell.execute_reply": "2025-05-28T01:28:22.683451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied assign_exam_date() using RIDEXMON.\n",
      "\n",
      "Successfully created and saved sessions.csv to ../data/clean/sessions.csv (with fasting data if available).\n",
      "Sessions.csv has 9254 rows.\n",
      "    UserID SessionDate  FastingStatus\n",
      "0  93703.0  2018-07-15              0\n",
      "1  93704.0  2018-01-15              0\n",
      "2  93705.0  2018-07-15              0\n",
      "3  93706.0  2018-07-15              1\n",
      "4  93707.0  2018-07-15              0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/8wxk37tn5dlcg09q_bz7zmtm0000gn/T/ipykernel_88573/3370105991.py:52: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_sessions['IsFasted'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# --- Processing FASTQX_J.XPT for sessions.csv ---\n",
    "\n",
    "df_fasting_raw = read_xpt(\"FASTQX_J.XPT\", limit_cols=['SEQN', 'PHAFSTHR', 'PHAFSTMN'])\n",
    "df_fasting_status = pd.DataFrame() # Initialize\n",
    "\n",
    "if not df_fasting_raw.empty and 'SEQN' in df_fasting_raw.columns:\n",
    "    df_fasting = df_fasting_raw.copy()\n",
    "\n",
    "    # Convert fasting hours and minutes to numeric, coercing errors to NaN\n",
    "    # NHANES uses special codes for \"Refused\" or \"Don't know\" which will become NaN here.\n",
    "    # PHAFSTHR: Hours (0-99)\n",
    "    # PHAFSTMN: Minutes (0-59)\n",
    "    for col in ['PHAFSTHR', 'PHAFSTMN']:\n",
    "        if col in df_fasting.columns:\n",
    "            df_fasting[col] = pd.to_numeric(df_fasting[col], errors='coerce')\n",
    "        else:\n",
    "            print(f\"Warning: Expected fasting column {col} not found in FASTQX_J.XPT. Setting to 0.\")\n",
    "            df_fasting[col] = 0 # Add column as 0 if missing to prevent later errors\n",
    "\n",
    "    # Fill NaN with 0 for calculation if you want to consider missing components as 0 duration\n",
    "    # Or, if any part is NaN, the total duration could be NaN. Let's default to 0.\n",
    "    df_fasting.fillna({'PHAFSTHR': 0, 'PHAFSTMN': 0}, inplace=True)\n",
    "\n",
    "    # Calculate total fasting time in hours\n",
    "    df_fasting['TotalFastingHours'] = df_fasting['PHAFSTHR'] + (df_fasting['PHAFSTMN'] / 60.0)\n",
    "\n",
    "    # Define \"fasting\" criteria (e.g., >= 8 hours is common for glucose tests)\n",
    "    # Your schema expects a BOOLEAN (0 or 1 for MySQL)\n",
    "    fasting_threshold_hours = 8\n",
    "    df_fasting['IsFasted'] = np.where(df_fasting['TotalFastingHours'] >= fasting_threshold_hours, 1, 0)\n",
    "    \n",
    "    # Select SEQN and the IsFasted column\n",
    "    df_fasting_status = df_fasting[['SEQN', 'IsFasted']].copy()\n",
    "    df_fasting_status.drop_duplicates(subset=['SEQN'], keep='first', inplace=True) # Should be one entry per SEQN\n",
    "\n",
    "else:\n",
    "    print(\"FASTQX_J.XPT could not be read or SEQN is missing. FastingStatus will use default.\")\n",
    "    # df_fasting_status will remain empty if the file isn't read\n",
    "\n",
    "# Creating & Updating sessions.csv\n",
    "if not df_users_final.empty and 'SEQN' in df_users_final.columns:\n",
    "    df_sessions = df_users_final[['SEQN']].copy()\n",
    "    # df_sessions.rename(columns={'SEQN': 'UserID_SEQN'}, inplace=True) # Using SEQN as the key for UserID\n",
    "    \n",
    "    df_sessions['SessionDate'] = populate_date(df_sessions, 'SessionDate')\n",
    "\n",
    "    # Merge with fasting status information\n",
    "    if not df_fasting_status.empty:\n",
    "        df_sessions = pd.merge(df_sessions, df_fasting_status, on='SEQN', how='left')\n",
    "        # If a participant from df_users_final is not in df_fasting_status, 'IsFasted' will be NaN.\n",
    "        # Fill NaN 'IsFasted' with a default (e.g., 0 for not fasted/unknown)\n",
    "        df_sessions['IsFasted'].fillna(0, inplace=True)\n",
    "        df_sessions['FastingStatus'] = df_sessions['IsFasted'].astype(int)\n",
    "        df_sessions.drop(columns=['IsFasted'], inplace=True) # Clean up intermediate column\n",
    "    else:\n",
    "        # Default if fasting data is unavailable\n",
    "        df_sessions['FastingStatus'] = 0 \n",
    "\n",
    "    # The schema `load.sh` expects columns named (UserID, SessionDate, FastingStatus)\n",
    "    # where UserID in the CSV refers to the User.SEQN (assuming UserID in DB = SEQN or handled by load script)\n",
    "    # The `longevity-biomarker-tracker/etl/load.sh` script loads `sessions.csv` columns (UserID, SessionDate, FastingStatus)\n",
    "    # So, the column in sessions.csv that holds the SEQN value should be named 'UserID'\n",
    "    df_sessions.rename(columns={'SEQN': 'UserID'}, inplace=True) # Rename SEQN to UserID for the CSV\n",
    "    \n",
    "    final_session_columns = ['UserID', 'SessionDate', 'FastingStatus']\n",
    "    # Ensure all necessary columns are present\n",
    "    existing_final_session_cols = [col for col in final_session_columns if col in df_sessions.columns]\n",
    "    \n",
    "    if len(existing_final_session_cols) == len(final_session_columns):\n",
    "        df_sessions_final = df_sessions[existing_final_session_cols]\n",
    "\n",
    "        if not df_sessions_final.empty:\n",
    "            sessions_csv_path = clean_data_dir / \"sessions.csv\"\n",
    "            df_sessions_final.to_csv(sessions_csv_path, index=False)\n",
    "            print(f\"\\nSuccessfully created and saved sessions.csv to {sessions_csv_path} (with fasting data if available).\")\n",
    "            print(f\"Sessions.csv has {len(df_sessions_final)} rows.\")\n",
    "            print(df_sessions_final.head())\n",
    "        else:\n",
    "            print(\"Could not generate sessions.csv (empty after processing).\")\n",
    "            df_sessions_final = pd.DataFrame() # Ensure it's defined\n",
    "    else:\n",
    "        missing_cols_for_sessions = set(final_session_columns) - set(existing_final_session_cols)\n",
    "        print(f\"Could not generate sessions.csv due to missing columns: {missing_cols_for_sessions}\")\n",
    "        df_sessions_final = pd.DataFrame() # Ensure it's defined\n",
    "        \n",
    "else:\n",
    "    df_sessions_final = pd.DataFrame()\n",
    "    print(\"Skipping sessions.csv creation as users.csv (df_users_final) is not available or SEQN is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8656bb9e",
   "metadata": {},
   "source": [
    "#### Processing for measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f9b9c91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:28:22.684839Z",
     "iopub.status.busy": "2025-05-28T01:28:22.684765Z",
     "iopub.status.idle": "2025-05-28T01:28:22.949871Z",
     "shell.execute_reply": "2025-05-28T01:28:22.949646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied assign_exam_date() using RIDEXMON.\n",
      "\n",
      "Successfully created and saved measurements.csv to ../data/clean/measurements.csv\n",
      "Measurements.csv has 57959 rows.\n",
      "\n",
      "--- ETL Transformation Script Complete ---\n",
      "CSVs generated in: ../data/clean\n",
      "Please review the generated CSVs.\n",
      "Note: The measurements.csv currently contains SEQN. The load.sh script will need to be adapted\n",
      "to look up the correct auto-generated SessionID from the MeasurementSession table based on this SEQN (as UserID) and SessionDate.\n"
     ]
    }
   ],
   "source": [
    "# --- Processing several XPTs for measurements.csv ---\n",
    "\n",
    "biomarker_map_nhanes_to_id = {\n",
    "    'LBXSAL': 1,  # Albumin\n",
    "    'LBXSAPSI': 2, # Alkaline Phosphatase\n",
    "    'LBXSCR': 3,  # Creatinine\n",
    "    'LBXGLU': 4,  # Fasting Glucose - This comes from GLU_J.XPT, others from BIOPRO_J/CBC_J\n",
    "    'LBXHSCRP': 5, # High-Sensitivity CRP - This comes from HSCRP_J.XPT\n",
    "    'LBXWBCSI': 6, # White Blood Cell Count\n",
    "    'LBXLYPCT': 7, # Lymphocyte Percentage\n",
    "    'LBXMCVSI': 8, # Mean Corpuscular Volume\n",
    "    'LBXRDW': 9    # Red Cell Distribution Width\n",
    "}\n",
    "# Create a reverse map for convenience if needed: id_to_nhanes_code\n",
    "id_to_nhanes_code = {v: k for k, v in biomarker_map_nhanes_to_id.items()}\n",
    "\n",
    "lab_files_config = {\n",
    "    \"BIOPRO_J.XPT\": ['SEQN', 'LBXSAL', 'LBXSAPSI', 'LBXSCR'], # Albumin, Alk Phos, Creatinine\n",
    "    \"GLU_J.XPT\": ['SEQN', 'LBXGLU'],                          # Fasting Glucose. Also check LBDGLUSI for SI units if preferred.\n",
    "                                                              # GLU_J also has WTSAF2YR (fasting subsample weight)\n",
    "                                                              # and PHAFSTHR/MN (fasting time) are in FASTQX_J\n",
    "    \"HSCRP_J.XPT\": ['SEQN', 'LBXHSCRP'],                      # HS CRP (variable name is indeed LBXHSCRP in docs)\n",
    "    \"CBC_J.XPT\": ['SEQN', 'LBXWBCSI', 'LBXLYPCT', 'LBXMCVSI', 'LBXRDW'] # WBC, Lymph %, MCV, RDW\n",
    "}\n",
    "\n",
    "all_measurements_list = []\n",
    "\n",
    "for file_name, nhanes_vars in lab_files_config.items():\n",
    "    df_lab_raw = read_xpt(file_name, limit_cols=nhanes_vars)\n",
    "    \n",
    "    if df_lab_raw.empty or 'SEQN' not in df_lab_raw.columns:\n",
    "        print(f\"Skipping {file_name} due to read error or missing SEQN.\")\n",
    "        continue\n",
    "        \n",
    "    # Melt the DataFrame to long format: SEQN, NHANESVarCode, Value\n",
    "    # Keep only SEQN and the actual biomarker columns present in nhanes_vars AND df_lab_raw\n",
    "    biomarker_cols_in_file = [var for var in nhanes_vars if var != 'SEQN' and var in df_lab_raw.columns]\n",
    "    if not biomarker_cols_in_file:\n",
    "        print(f\"No specified biomarker columns found in {file_name}.\")\n",
    "        continue\n",
    "\n",
    "    df_melted = df_lab_raw.melt(id_vars=['SEQN'], \n",
    "                                value_vars=biomarker_cols_in_file,\n",
    "                                var_name='NHANESVarCode', \n",
    "                                value_name='Value')\n",
    "    \n",
    "    # Drop rows where Value is NaN (missing measurement)\n",
    "    df_melted.dropna(subset=['Value'], inplace=True)\n",
    "    \n",
    "    if df_melted.empty:\n",
    "        print(f\"No valid measurements after melting and dropping NaNs for {file_name}.\")\n",
    "        continue\n",
    "\n",
    "    all_measurements_list.append(df_melted)\n",
    "\n",
    "if all_measurements_list:\n",
    "    df_all_measurements = pd.concat(all_measurements_list, ignore_index=True)\n",
    "    \n",
    "    # Map NHANESVarCode to BiomarkerID\n",
    "    df_all_measurements['BiomarkerID'] = df_all_measurements['NHANESVarCode'].map(biomarker_map_nhanes_to_id)\n",
    "    \n",
    "    # Drop rows where BiomarkerID could not be mapped (i.e., NHANESVarCode not in our map)\n",
    "    df_all_measurements.dropna(subset=['BiomarkerID'], inplace=True)\n",
    "    df_all_measurements['BiomarkerID'] = df_all_measurements['BiomarkerID'].astype(int)\n",
    "    \n",
    "    # Add TakenAt date\n",
    "    df_all_measurements['TakenAt'] =  populate_date(df_all_measurements, 'TakenAt')\n",
    "        \n",
    "    df_measurements_final = df_all_measurements[['SEQN', 'BiomarkerID', 'Value', 'TakenAt']].copy()\n",
    "    df_measurements_final.dropna(inplace=True) # Ensure all rows are complete\n",
    "\n",
    "    if not df_measurements_final.empty:\n",
    "        measurements_csv_path = clean_data_dir / \"measurements.csv\"\n",
    "        df_measurements_final.to_csv(measurements_csv_path, index=False)\n",
    "        print(f\"\\nSuccessfully created and saved measurements.csv to {measurements_csv_path}\")\n",
    "        print(f\"Measurements.csv has {len(df_measurements_final)} rows.\")\n",
    "    else:\n",
    "        print(\"Could not generate measurements.csv due to missing data or mapping issues.\")\n",
    "else:\n",
    "    print(\"No lab data processed for measurements.csv.\")\n",
    "\n",
    "print(\"\\n--- ETL Transformation Script Complete ---\")\n",
    "print(f\"CSVs generated in: {clean_data_dir}\")\n",
    "print(\"Please review the generated CSVs.\")\n",
    "print(\"Note: The measurements.csv currently contains SEQN. The load.sh script will need to be adapted\")\n",
    "print(\"to look up the correct auto-generated SessionID from the MeasurementSession table based on this SEQN (as UserID) and SessionDate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5e93ce7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T15:22:37.931472Z",
     "start_time": "2025-05-22T15:22:37.746886Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-28T01:28:22.951020Z",
     "iopub.status.busy": "2025-05-28T01:28:22.950919Z",
     "iopub.status.idle": "2025-05-28T01:28:23.059332Z",
     "shell.execute_reply": "2025-05-28T01:28:23.059105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Applying final patches for database compatibility ===\n",
      "✓ Updated users.csv with BirthDate column\n",
      "✓ Updated sessions.csv with SEQN column\n",
      "✓ Updated measurements.csv with SessionDate column\n",
      "\n",
      "=== CSV Compatibility Patches Complete ===\n",
      "All CSVs should now load successfully with load.sh\n",
      "Files updated in: ../data/clean\n",
      "⏩  Final users.csv patch – order & SEQN dtype\n",
      "✓ users.csv fixed → ../data/clean/users.csv\n",
      "\n",
      "users.csv columns: ['SEQN', 'BirthDate', 'Sex', 'RaceEthnicity']\n",
      "  Rows: 9254\n",
      "\n",
      "sessions.csv columns: ['SEQN', 'SessionDate', 'FastingStatus']\n",
      "  Rows: 9254\n",
      "\n",
      "measurements.csv columns: ['SEQN', 'SessionDate', 'BiomarkerID', 'Value', 'TakenAt']\n",
      "  Rows: 57959\n",
      "\n",
      "anthropometry.csv columns: ['SEQN', 'ExamDate', 'HeightCM', 'WeightKG', 'BMI']\n",
      "  Rows: 8704\n"
     ]
    }
   ],
   "source": [
    "# === FINAL PATCH CELL: Fix CSV formats for database loading ===\n",
    "# (Added to make CSVs compatible with load.sh without changing existing logic)\n",
    "\n",
    "print(\"=== Applying final patches for database compatibility ===\")\n",
    "\n",
    "# A. Fix users.csv: Replace Age with BirthDate\n",
    "if 'df_users_final' in globals() and not df_users_final.empty:\n",
    "    CYCLE_YEAR = 2018  # NHANES J cycle is 2017-2018\n",
    "\n",
    "    def derive_birthdate(age):\n",
    "        return f\"{CYCLE_YEAR - int(age)}-07-01\"  # Mid-year birthdate\n",
    "\n",
    "    df_users_final[\"BirthDate\"] = df_users_final[\"Age\"].apply(derive_birthdate)\n",
    "    df_users_final.drop(columns=[\"Age\"], inplace=True)\n",
    "\n",
    "    # Re-export users.csv\n",
    "    users_csv_path = clean_data_dir / \"users.csv\"\n",
    "    df_users_final.to_csv(users_csv_path, index=False)\n",
    "    print(f\"✓ Updated users.csv with BirthDate column\")\n",
    "\n",
    "# B. Fix sessions.csv: Change UserID back to SEQN\n",
    "if 'df_sessions_final' in globals() and not df_sessions_final.empty:\n",
    "    df_sessions_final.rename(columns={\"UserID\": \"SEQN\"}, inplace=True)\n",
    "\n",
    "    # Re-export sessions.csv\n",
    "    sessions_csv_path = clean_data_dir / \"sessions.csv\"\n",
    "    df_sessions_final.to_csv(sessions_csv_path, index=False)\n",
    "    print(f\"✓ Updated sessions.csv with SEQN column\")\n",
    "\n",
    "# C. Fix measurements.csv: Add SessionDate for join\n",
    "if 'df_measurements_final' in globals() and not df_measurements_final.empty:\n",
    "    # Add SessionDate (same as TakenAt for simplicity)\n",
    "    df_measurements_final[\"SessionDate\"] = df_measurements_final[\"TakenAt\"]\n",
    "\n",
    "    # Reorder columns for load.sh\n",
    "    final_cols = [\"SEQN\", \"SessionDate\", \"BiomarkerID\", \"Value\", \"TakenAt\"]\n",
    "    df_measurements_final = df_measurements_final[final_cols]\n",
    "\n",
    "    # Re-export measurements.csv\n",
    "    measurements_csv_path = clean_data_dir / \"measurements.csv\"\n",
    "    df_measurements_final.to_csv(measurements_csv_path, index=False)\n",
    "    print(f\"✓ Updated measurements.csv with SessionDate column\")\n",
    "\n",
    "print(\"\\n=== CSV Compatibility Patches Complete ===\")\n",
    "print(\"All CSVs should now load successfully with load.sh\")\n",
    "print(f\"Files updated in: {clean_data_dir}\")\n",
    "\n",
    "# === D: align users.csv with load.sh column order ===\n",
    "print(\"⏩  Final users.csv patch – order & SEQN dtype\")\n",
    "\n",
    "if \"df_users_final\" in globals() and not df_users_final.empty:\n",
    "    # Ensure BirthDate already exists (previous patch)\n",
    "    if \"BirthDate\" not in df_users_final.columns:\n",
    "        raise ValueError(\"BirthDate column missing – run earlier cells first\")\n",
    "\n",
    "    # 1) Cast SEQN to int so we don't get 93703.0 in the CSV\n",
    "    df_users_final[\"SEQN\"] = df_users_final[\"SEQN\"].astype(int)\n",
    "\n",
    "    # 2) Re-order columns to match LOAD DATA list\n",
    "    df_users_final = df_users_final[[\"SEQN\", \"BirthDate\", \"Sex\", \"RaceEthnicity\"]]\n",
    "\n",
    "    # 3) Overwrite the CSV\n",
    "    users_csv_path = clean_data_dir / \"users.csv\"\n",
    "    df_users_final.to_csv(users_csv_path, index=False)\n",
    "    print(\"✓ users.csv fixed →\", users_csv_path)\n",
    "\n",
    "# Display final column structure for verification\n",
    "for csv_file in [\"users.csv\", \"sessions.csv\", \"measurements.csv\", \"anthropometry.csv\"]:\n",
    "    csv_path = clean_data_dir / csv_file\n",
    "    if csv_path.exists():\n",
    "        df_check = pd.read_csv(csv_path)\n",
    "        print(f\"\\n{csv_file} columns: {list(df_check.columns)}\")\n",
    "        print(f\"  Rows: {len(df_check)}\")\n",
    "    else:\n",
    "        print(f\"\\n{csv_file}: Not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b1fcee10ebc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ea2856c07bed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
